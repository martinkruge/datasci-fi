---
title: "Assignment 2 <br> Sentiment Analysis - Topic Modelling - Shiny App"
author: "Martin Kruger (KRGMAR043)"
date: '`r format(Sys.Date())`'
output:
  html_document:
    toc_depth: '2'
---

```{r setup, include=FALSE}

library(knitr)
knitr::opts_chunk$set(
	fig.pos = "H",
	include = FALSE,
	tidy = TRUE
)

library(tidyverse)
library(tidytext)
library(stringr)
library(lubridate)
library(ggplot2)
library(topicmodels)

# Set seed for repeatable results
set.seed(790904)

options(repr.plot.width=4, repr.plot.height=3) # set plot size in the notebook

load(file = "./CFPB/output/CFPB_sentiment.RData", globalenv()) # Workaround to avoid re-running LDA for knit
load(file = "./CFPB/output/CFPB_tidy.RData", globalenv()) # Workaround to avoid re-running LDA for knit
load(file = "./CFPB/output/CFPB_LDA.RData", globalenv()) # Workaround to avoid re-running LDA for knit
load(file = "./CFPB/output/CFPB_LDA_k_beta.RData", globalenv()) # Workaround to avoid re-running LDA for knit
load(file = "./CFPB/output/CFPB_LDA_k_gamma.RData", globalenv()) # Workaround to avoid re-running LDA for knit

```

***

### 1. Introduction

The aim of the project is to build a Shiny app that will allow users to do some text mining of complaints data coming from the US Consumer Financial Protection Bureau. 

This markdown document was put together during the developemnt of the Shiny app to do data preparation and to get pieces of code working before implementing in the Shiny app.

It will also serve as documentation to explain how to use the app by giving some worked examples.


### 2. Data Description

Each record/row captures a single complaint, on the following variables:

 + *date_received*: the date the complaint was received  
 + *product*: a broad categorisation of the type of financial product involved   
       - 5 categories:  
         1. bank account or service  
         2. credit card  
         3. credit reporting  
         4. debt collection, mortgage    
 + *consumer_complaint_narrative*: the submitted text explaining the nature of the complaint.  
 + *consumer_compensated*: a binary indicator of whether the consumer was compensated (monetarily or non-monetarily) after the complaint.  


```{r data_load, include=FALSE}

# This loads raw data provided 
#rm(list=ls()) # Clean up environment


# Load data from .RData
data.file <- "./CFPB/data/complaints.RData"

load(data.file)

complaints$product <- as.factor(complaints$product)
complaints$date_received <- as.Date(complaints$date_received, format = "%m/%d/%Y")

complaints <- as.tibble(complaints)

complaints <- complaints %>% rename(date = date_received)                     # Rename `date_received' 
complaints <- complaints %>% rename(narrative = consumer_complaint_narrative) # Rename `consumer_complaint_narrative'
complaints <- complaints %>% rename(compensated = consumer_compensated)       # Rename `consumer_compensated`

products <- levels(complaints$product)

```


### 3. Data exploration

```{r}

# Exploring data to get an idea of the content
complaints.by_date <-  complaints %>% 
  group_by(date, product) %>% 
  summarise(count = n())  # Count of complaints per day

# Plot complaints per day by product
ggplot(data = complaints.by_date, aes(x = date, y = count, fill = product)) + 
  geom_col(show.legend = FALSE) + 
  ggtitle("Complaints per product per day") +
  facet_wrap(~product, ncol = 2, scales = "free_y") 

```

```{r}

complaints.by_compensated <-  complaints %>% 
  group_by(date, compensated) %>% 
  summarise(count = n()) %>%
  left_join(complaints %>% group_by(date) %>% summarise(total = sum(n()))) %>%
  mutate(ratio = count/total)

# Plot complaints per day by compensation variable
ggplot(data = complaints.by_compensated, aes(x = date, y = count, fill = compensated)) + 
  geom_col() + 
  #geom_line(data=complaints.by_compensated, aes(x = date, y = ratio), colour = "black", type = 2) +  # Add rate
  ggtitle("Complaints compensated vs uncompensated") +
  facet_wrap(~compensated, ncol = 1, scales = "free_y") 

```


```{r}

complaints.by_product_compensated <- complaints %>% 
  group_by(date, product, compensated) %>% 
  summarise(count = n()) %>%
  ungroup()


# Plot per product the compensated vs uncompensated counts
ggplot(data = complaints.by_product_compensated, aes(x = date, y = count, fill = compensated)) + 
  geom_col() + # show.legend = FALSE
  ggtitle("Complaints compensated vs uncompensated per product") +
  labs(x = "Date", y = "Count") +
  facet_wrap(~product , ncol = 2, scales = "free_y") 

```



### 4. Sentiment analysis

```{r include=FALSE}

# Make tidy data
#----------------
replace_reg <- "X{4}[/:]?|X{2}[/:]?|[^A-Za-z\\d\\s]"  # Regex for text to be removed, like XXXX, XX/

# Create tidy data by 1st unnesting by sentences
# Replacing regex above in each sentence
# Unnesting by words, label each sentence
tidy.complaints <- complaints %>%
  unnest_tokens(text, narrative, token = "sentences", to_lower=F) %>%
  mutate(text = str_replace_all(text, replace_reg, "")) %>%
  group_by(id) %>%
  mutate(sentence = row_number()) %>%
  ungroup() %>%
  unnest_tokens(word, text, token = "words") 


# Add sentiment per word from 'bing' lexicon
#--------------------------------------------
tidy.complaints <- tidy.complaints %>%
  #filter(!word %in% stop_words$word, str_detect(word, "[a-z]")) %>% # remove stop words - DON'T
  left_join(get_sentiments("bing")) %>% # add sentiments (pos or neg)
  select(word, sentiment, everything()) %>%
  mutate(sentiment = ifelse(is.na(sentiment), "neutral", sentiment))


save(file = "./CFPB/output/CFPB_tidy.RData", 
     list=c("tidy.complaints" ))

```


```{r}

# Most used words per sentiment
tidy.complaints %>%
  filter(sentiment %in% c("positive","negative")) %>%
  group_by(sentiment) %>%
  count(word) %>%
  arrange(desc(n)) %>%
  filter(rank(desc(n)) <= 20) %>%
  ungroup() %>%
  ggplot(aes(reorder(word,n),n)) + 
  ggtitle("Most used words per sentiment") +
  geom_col(show.legend = FALSE) + 
  coord_flip() + 
  xlab("") + 
  facet_wrap(~sentiment, ncol = 2, scales = "free") 

```



```{r}

# Most used words per sentiment - per compensation
tidy.complaints %>%
  filter(sentiment %in% c("positive","negative")) %>%
  group_by(sentiment, compensated) %>%
  count(word) %>%
  ungroup() %>%
  arrange(desc(n)) %>%
  group_by(sentiment, word) %>%
  mutate(total = sum(n)) %>%
  ungroup() %>%
  arrange(desc(total),desc(n)) %>%
  group_by(sentiment, compensated) %>%
  mutate(rank = rank(desc(total),desc(n))) %>%
  ungroup() %>%
  filter(rank <= 20) %>%
  ggplot(aes(reorder(word,n),n, fill = compensated)) +
  ggtitle("Most used words per sentiment") +
  geom_col() + 
  coord_flip() + xlab("") + 
  facet_wrap(~sentiment, ncol = 2, scales = "free") 

```

#### Aggregated sentiment per complaint

```{r}

# Sum word sentiments over complaint to get net-sentiment
sentiment_per_complaint <-  tidy.complaints %>% 
  filter(sentiment %in% c("positive","negative")) %>%
  group_by(date, id, product, compensated) %>%
  summarize(net_sentiment = (sum(sentiment == "positive") - sum(sentiment == "negative"))) %>%
  ungroup()

```
\ 


#### Dealing with negation 

```{r, include=FALSE}

replace_reg <- "X{4}[/:]?|X{2}[/:]?|[^A-Za-z\\d\\s]"  # Regex for text to be removed, like XXXX, XX/

# Create tidy data by replacing regex above
# Unnesting by bigrams, seperate by space
bigrams_separated <- complaints %>%
  mutate(text = str_replace_all(narrative, replace_reg, "")) %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  select(-narrative)

# remove stop words - DON'T
#bigrams_filtered <- bigrams_separated %>%
#  filter(!word1 %in% stop_words$word) %>%
#  filter(!word2 %in% stop_words$word)

# add sentiment for each word
#bigrams_filtered <- bigrams_filtered %>% 
bigrams_separated <- bigrams_separated %>% 
    # add sentiment for word 1
    left_join(get_sentiments("bing"), by = c(word1 = "word")) %>%
    rename(sentiment1 = sentiment) %>%
    mutate(sentiment1 = ifelse(is.na(sentiment1), "neutral", sentiment1)) %>%
    
    # add sentiment for word 1
    left_join(get_sentiments("bing"), by = c(word2 = "word")) %>%
    rename(sentiment2 = sentiment) %>%
    mutate(sentiment2 = ifelse(is.na(sentiment2), "neutral", sentiment2)) %>%
    select(date,word1,word2,sentiment1,sentiment2,everything())

```


Now we need a list of words that we consider to be negation words. Use the following set, taken from TMR [Chapter 4](http://tidytextmining.com/ngrams.html), and show a few examples.

```{r}

negation_words <- c("not", "no", "never", "without")

# show a few
filter(bigrams_separated, word1 %in% negation_words) %>% 
  filter(!word2 %in% stop_words$word) %>%
    head(10) %>% select(date, word1, word2, sentiment1, sentiment2) # for display purposes
```

We now reverse the sentiment of `word2` whenever it is preceded by a negation word, and then add up the number of positive and negative words within a bigram and take the difference. That difference (a score from -2 to +2) is the sentiment of the bigram.

We do this in two steps for illustrative purposes. First we reverse the sentiment of the second word in the bigram if the first one is a negation word.

```{r}

bigrams_separated.negated <- bigrams_separated %>%

    # create a variable that is the opposite of sentiment2
    mutate(opp_sentiment2 = recode(sentiment2, "positive" = "negative",
                                 "negative" = "positive",
                                 "neutral" = "neutral")) %>%
    
    # reverse sentiment2 if word1 is a negation word
    mutate(sentiment2 = ifelse(word1 %in% negation_words, opp_sentiment2, sentiment2)) %>%
    # remove the opposite sentiment variable, which we don't need any more
    select(-opp_sentiment2)


filter(bigrams_separated.negated, word1 %in% negation_words) %>%
  filter(sentiment2 %in% c("positive","negative"))

```

Next, we calculate the sentiment of each bigram and join up the words in the bigram again. Only retain the one's where the net effect is not neutral.


```{r}

bigrams_separated.negated <- bigrams_separated.negated %>%
  mutate(net_sentiment = (sentiment1 == "positive") + (sentiment2 == "positive") - 
              (sentiment1 == "negative") - (sentiment2 == "negative")) %>%
  unite(bigram, word1, word2, sep = " ", remove = FALSE) %>% 
  filter(net_sentiment != 0) %>%
  filter(!word2 %in% stop_words$word) %>% # remove cases where word2 is a stop word
  filter(word1 %in% negation_words) %>% # only keep cases where negation took place
  select(-word1, -word2, -sentiment1, -sentiment2)  # remove columns to prevent re-negation

bigrams_separated.negated 

```


Most common positive and negative bigrams with negation. 

```{r}

bigrams_separated.negated %>%
  filter(net_sentiment > 0) %>% # get positive bigrams
  count(bigram, sort = TRUE) %>%
  filter(rank(desc(n)) < 20) %>%
  ggplot(aes(reorder(bigram,n),n)) + 
  ggtitle("Most common positive bigrams with negation") +
  geom_col() + coord_flip() + xlab("")

```


```{r}

bigrams_separated.negated %>%
  filter(net_sentiment < 0) %>% # get negative bigrams
  count(bigram, sort = TRUE) %>%
  filter(rank(desc(n)) < 20) %>%
  ggplot(aes(reorder(bigram,n),n)) + 
  ggtitle("Most common negative bigrams with negation") +
  geom_col() + coord_flip() + xlab("")

```
\ 

Calculate per complaint a net-effect of negation by summing the net-sentiment over bigrams per complaint.

```{r}

complaints.negated <- bigrams_separated.negated %>%
  filter(net_sentiment != 0) %>% # get non-neutral bigrams
  group_by(id) %>%
  summarise(net_sentiment = sum(net_sentiment)) %>%
  filter(net_sentiment != 0)

```


Add to `sentiment_per_complaint` and recalculate the net-sentiment.

```{r}

sentiment_per_complaint <- sentiment_per_complaint %>% 
  left_join(complaints.negated %>% rename(neg.net_sentiment = net_sentiment), by = c("id")) %>%
  mutate(net_sentiment.new = (net_sentiment + ifelse(is.na(neg.net_sentiment),0, 2*neg.net_sentiment))) # x2 = cancel old sentiment and add negated  (positive -> neutral -> negative / negative -> neutral -> positive)
  
```

***

\ 

Negative sentiment complaints samples where negation had a positive effect. 

```{r}

complaints %>% 
    left_join(sentiment_per_complaint) %>% 
    select(narrative, net_sentiment, net_sentiment.new) %>%
    filter(net_sentiment < 0) %>% # only negative samples
    mutate(negation_effect = net_sentiment.new - net_sentiment ) %>%
    filter(negation_effect > 0) %>%
    arrange(desc(negation_effect)) %>%
    head(10)

```

\ 

Positive sentiment complaints samples where negation had a negative effect. 

```{r}

complaints %>% 
    left_join(sentiment_per_complaint) %>% 
    select(narrative, net_sentiment, net_sentiment.new) %>%
    filter(net_sentiment > 0) %>%  # only positive samples
    mutate(negation_effect = net_sentiment.new - net_sentiment ) %>%
    filter(negation_effect < 0) %>%
    arrange(negation_effect) %>%
    head(10)
 
```
\ 

#### Plot sentiment per product  

```{r, fig.height=12, fig.width=10}

sentiment.summary <- sentiment_per_complaint %>% 
  select(-net_sentiment, -neg.net_sentiment) %>%
  mutate(net_sentiment = net_sentiment.new) %>%
  select(-net_sentiment.new) %>%
  #rename(net_sentiment = net_sentiment.new) %>%
  group_by(product, compensated,net_sentiment) %>%
  summarise(count=n()) %>%
  ungroup()


sentiment.summary %>%
  filter(product %in% c(products), compensated %in% c(T,F)) %>%
  ggplot(aes(x = net_sentiment, y = count, fill = product)) + 
  geom_col() + # show.legend = FALSE
  ggtitle("Sentiment histogram by product") +
  xlim(c(-20,20)) +
  labs(x = "Sentiment", y = "Frequency") +
  facet_wrap(~product+compensated , ncol = 2, scales = "free") 


# Save sentiment data for use in the app
save(file = "./CFPB/output/CFPB_sentiment.RData", 
     list=c("sentiment_per_complaint","sentiment.summary","products" ))
```


```{r}

sentiment_per_complaint %>%
  ggplot(aes(x=net_sentiment.new)) + 
  ggtitle("Sentiment histogram - all data") +
  #geom_histogram(binwidth=.5)
  geom_histogram(binwidth=1, alpha=.5, fill="darkgreen") +
  xlim(c(-30,30)) + 
  xlab("Sentiment") + 
  ylab("Complaint count") +
  theme(legend.position="none") 
  
  #ggplot(aes(x=net_sentiment.new, fill=product)) +
  #geom_histogram(binwidth=.5, alpha=.5, position="dodge")
  #geom_density(alpha=.3)


```

*** 



### 5. Topic modelling

Create *tidy* data by counting the number of ocurrences of each word in our vocabulary was used in each complaint, creating a "long" (and tidy) format of the document-term matrix. 


```{r include=FALSE}
# Create tidy complaints data - count of each word per complaint id
complaints_tdf <- tidy.complaints %>%
  filter(!word %in% stop_words$word, str_detect(word, "[a-z]")) %>% # Remove stopwords and numbers
  group_by(id, word) %>%
  count() %>%  
  ungroup()
```


Inspect for funny words... on the topN and lowN sides

```{r}
tidy.complaints %>% 
  #filter(!word %in% stop_words$word, str_detect(word, "^[a-z]")) %>% # Remove stopwords and non-words
  filter(!word %in% stop_words$word, str_detect(word, "[a-z]")) %>% # Remove stopwords and numbers
  group_by(word) %>%
  count() %>%  
  ungroup() %>%
  #arrange(desc(n))
  arrange(n)

```


Create a *ducument-term matrix*. The **topicmodels** package, which we use to implement the LDA topic model, requires the document-term matrix to be provided as a `DocumentTermMatrix` object. 


```{r}  
# Create a document-term matrix with cast
complaints_dtm <- complaints_tdf %>% 
  cast_dtm(id, word, n)
```

We can now estimate the parameters of the topic model using LDA. We use the `LDA()` function provided by the package **topicmodelling**. We need to specify the number of latent variables (topics) we wish to use.

```{r, eval=FALSE, include=FALSE}

# Perform LDA with 5 topics - logical choice because we have 5 products... but in shiny we will "test" other values
complaints_lda_5 <- LDA(complaints_dtm, k = 5)  # 5 products
complaints_lda_5

# Create objects for other values of k for use in shiny app - LDA runs for a long time
complaints_lda_4 <- LDA(complaints_dtm, k = 4)  # 
complaints_lda_3 <- LDA(complaints_dtm, k = 3)  # 
complaints_lda_2 <- LDA(complaints_dtm, k = 2)  # 

save(file = "./CFPB/output/CFPB_LDA.RData", 
     list=c("complaints_tdf","complaints_dtm","complaints_lda_5","complaints_lda_4","complaints_lda_3","complaints_lda_2" ))

str(complaints_lda_5)
str(complaints_lda_4)
str(complaints_lda_3)
str(complaints_lda_2)

```

* `beta`: these parameters control the *probability of a given topic k generating a particular word i.* 
* `gamma`: this gives the *topic "mixtures" for each document.* 


```{r, eval=FALSE, warning=FALSE, include=FALSE}

# tidy way of getting topic betas 
complaint_topics_5 <- tidy(complaints_lda_5, matrix = "beta")
complaint_topics_4 <- tidy(complaints_lda_4, matrix = "beta")
complaint_topics_3 <- tidy(complaints_lda_3, matrix = "beta")
complaint_topics_2 <- tidy(complaints_lda_2, matrix = "beta")

# Top15 terms per topic - for each LDA k setting
top_terms_5 <- complaint_topics_5 %>%  group_by(topic) %>%  top_n(15, beta) %>%  ungroup() %>%  arrange(topic, -beta)
top_terms_4 <- complaint_topics_4 %>%  group_by(topic) %>%  top_n(15, beta) %>%  ungroup() %>%  arrange(topic, -beta)
top_terms_3 <- complaint_topics_3 %>%  group_by(topic) %>%  top_n(15, beta) %>%  ungroup() %>%  arrange(topic, -beta)
top_terms_2 <- complaint_topics_2 %>%  group_by(topic) %>%  top_n(15, beta) %>%  ungroup() %>%  arrange(topic, -beta)

save(file = "./CFPB/output/CFPB_TopTerms.RData", 
     list=c("top_terms_5","top_terms_4","top_terms_3","top_terms_2" ))

```


Top 15 terms per topic for each of the k settings that LDA was done for.

```{r, echo=TRUE, message=FALSE, warning=FALSE}

library(ggplot2)

top_terms_5 %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

```

```{r}

top_terms_4 %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

```

```{r}

top_terms_3 %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

```

```{r}

top_terms_2 %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

```


Long way of getting topic betas - this time create a tidy dataset for easy comparison later on...

```{r}

# The long way to get topic beta values...

term5 <- as.character(complaints_lda_5@terms)
topic1 <- complaints_lda_5@beta[1,]
topic2 <- complaints_lda_5@beta[2,]
topic3 <- complaints_lda_5@beta[3,]
topic4 <- complaints_lda_5@beta[4,]
topic5 <- complaints_lda_5@beta[5,]
k <- rep(5, length(term5))

complaint_topics5 <- tibble(k=k, term = term5, topic1 = topic1, topic2 = topic2, topic3 = topic3, topic4 = topic4, topic5 = topic5)


term4 <- as.character(complaints_lda_4@terms)
topic1 <- complaints_lda_4@beta[1,]
topic2 <- complaints_lda_4@beta[2,]
topic3 <- complaints_lda_4@beta[3,]
topic4 <- complaints_lda_4@beta[4,]
k <- rep(4, length(term4))

complaint_topics4 <- tibble(k=k, term = term4, topic1 = topic1, topic2 = topic2, topic3 = topic3, topic4 = topic4)


term3 <- as.character(complaints_lda_3@terms)
topic1 <- complaints_lda_3@beta[1,]
topic2 <- complaints_lda_3@beta[2,]
topic3 <- complaints_lda_3@beta[3,]
k <- rep(3, length(term3))

complaint_topics3 <- tibble(k=k, term = term3, topic1 = topic1, topic2 = topic2, topic3 = topic3)


term2 <- as.character(complaints_lda_2@terms)
topic1 <- complaints_lda_2@beta[1,]
topic2 <- complaints_lda_2@beta[2,]
k <- rep(2, length(term2))

complaint_topics2 <- tibble(k=k, term = term2, topic1 = topic1, topic2 = topic2)


# tidy set of topic beta values for each k that LDA was done for
complaint_topics5 <- complaint_topics5 %>% 
  gather(topic1, topic2, topic3, topic4, topic5, key = "topic", value = "beta") %>%
  mutate(beta = exp(beta)) # pr(topic k generates word i) = exp(beta_ik)

complaint_topics4 <- complaint_topics4 %>% 
  gather(topic1, topic2, topic3, topic4, key = "topic", value = "beta") %>%
  mutate(beta = exp(beta)) # pr(topic k generates word i) = exp(beta_ik)

complaint_topics3 <- complaint_topics3 %>% 
  gather(topic1, topic2, topic3, key = "topic", value = "beta") %>%
  mutate(beta = exp(beta)) # pr(topic k generates word i) = exp(beta_ik)

complaint_topics2 <- complaint_topics2 %>% 
  gather(topic1, topic2, key = "topic", value = "beta") %>%
  mutate(beta = exp(beta)) # pr(topic k generates word i) = exp(beta_ik)


# One big tidy table with all k setting topic betas
complaint_topics <- bind_rows(complaint_topics5, complaint_topics4, complaint_topics3, complaint_topics2)

# Save data as .RData for re-use in app
save(file = "./CFPB/output/CFPB_LDA_k_beta.RData", 
     list=c("complaint_topics" ))

```




Per complaint classification. 

```{r}

# Gamma matrix per LDA - for 5,4,3,2 topic cases 
complaint_gamma_5 <- tidy(complaints_lda_5, matrix = "gamma")
complaint_gamma_4 <- tidy(complaints_lda_4, matrix = "gamma")
complaint_gamma_3 <- tidy(complaints_lda_3, matrix = "gamma")
complaint_gamma_2 <- tidy(complaints_lda_2, matrix = "gamma")

complaint_gamma_5$k = 5
complaint_gamma_4$k = 4
complaint_gamma_3$k = 3
complaint_gamma_2$k = 2

complaint_gamma <- bind_rows(complaint_gamma_5, complaint_gamma_4, complaint_gamma_3, complaint_gamma_2)

# Save data as .RData for re-use in app
save(file = "./CFPB/output/CFPB_LDA_k_gamma.RData", 
     list=c("complaint_gamma" ))

```



```{r}
beta_spread <- complaint_topics %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001 | topic3 > .001 | topic4 > .001 | topic5 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

```


Construct a function that will do sentiment analysis of a given string.

```{r}


# Function to calculate sentiment of a string
str.Sentiment <- function(string, replace_reg, lexicon) {
  
  str.tib <- tibble(id = 0, text = as.character(string))
  
  # Create tidy data by unnesting by words
  
  net_sentiment <- str.tib %>%
    mutate(text = str_replace_all(text, replace_reg, "")) %>%
    unnest_tokens(word, text, token = "words") %>%
    group_by(id, word) %>%
    summarise(count = n()) %>%
    ungroup() %>%
    left_join(get_sentiments(lexicon)) %>% # add sentiments (pos or neg)
    select(word, sentiment, everything()) %>%
    mutate(sentiment = ifelse(is.na(sentiment), "neutral", sentiment)) %>%
    filter(sentiment %in% c("positive","negative")) %>%
    group_by(id) %>%
    summarise(net_sentiment = (sum(sentiment == "positive") - sum(sentiment == "negative"))) %>%
    ungroup() %>%
    select(net_sentiment) %>% 
    as.numeric()
  
}

# Function to calculate the Negated sentiment of a string
str.NegSentiment <- function(string, replace_reg, lexicon, negation_words) {
  
  str.tib <- tibble(id = 0, text = as.character(string))
  
  # Create bigrams to handle negation
  # Negate sentiment where relevant and compute neg.net_sentiment
 neg.net_sentiment <- str.tib %>%
    mutate(text = str_replace_all(text, replace_reg, "")) %>%
    unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
    separate(bigram, c("word1", "word2"), sep = " ") %>%
    
    # add sentiment for word 1
    left_join(get_sentiments(lexicon), by = c(word1 = "word")) %>%
    rename(sentiment1 = sentiment) %>%
    mutate(sentiment1 = ifelse(is.na(sentiment1), "neutral", sentiment1)) %>%
    
    # add sentiment for word 2
    left_join(get_sentiments(lexicon), by = c(word2 = "word")) %>%
    rename(sentiment2 = sentiment) %>%
    mutate(sentiment2 = ifelse(is.na(sentiment2), "neutral", sentiment2)) %>%
    select(word1,word2,sentiment1,sentiment2,everything()) %>%
    
    # create a variable that is the opposite of sentiment2
    mutate(opp_sentiment2 = recode(sentiment2, "positive" = "negative",
                                 "negative" = "positive",
                                 "neutral" = "neutral")) %>%
    
    # reverse sentiment2 if word1 is a negation word
    mutate(sentiment2 = ifelse(word1 %in% negation_words, opp_sentiment2, sentiment2)) %>%
    # remove the opposite sentiment variable, which we don't need any more
    select(-opp_sentiment2) %>%
    mutate(net_sentiment = (sentiment1 == "positive") + (sentiment2 == "positive") - 
              (sentiment1 == "negative") - (sentiment2 == "negative")) %>%
    #unite(bigram, word1, word2, sep = " ", remove = FALSE) %>% 
    filter(!word2 %in% stop_words$word) %>% # remove cases where word2 is a stop word
    filter(word1 %in% negation_words) %>% # only keep cases where negation took place
    group_by(id) %>%
    summarise(net_sentiment = sum(net_sentiment)) %>%
    ungroup() %>%
    select(net_sentiment) %>% 
    as.numeric()
    
    neg.net_sentiment <- ifelse(is.na(neg.net_sentiment), 0, 2*neg.net_sentiment)
}

# Function to calculate the Net sentiment of a string
str.NetSentiment <- function(string, replace_reg, lexicon, negation_words) {
   str.Sentiment(string, replace_reg, lexicon) + 
    str.NegSentiment(string, replace_reg, lexicon, negation_words)
}


replace_reg <- "X{4}[/:]?|X{2}[/:]?|[^A-Za-z\\d\\s]"  # Regex for text to be removed, like XXXX, XX/
negation_words <- c("not", "no", "never", "without")


# Test the functions on some strings.

myNegativeText <- "I hate to be left out in the dark."
myPositiveText <- "I am happy with the good service."
myNegPositiveText <- "I am not happy with the service."
myPosNegativeText <- "I do not hate the service."


(str.Sentiment(myNegativeText, replace_reg, lexicon = "bing"))
(str.Sentiment(myPositiveText, replace_reg, lexicon = "bing"))  
(str.Sentiment(myNegPositiveText, replace_reg, lexicon = "bing"))  
(str.Sentiment(myPosNegativeText, replace_reg, lexicon = "bing"))  

(str.NegSentiment(myNegativeText, replace_reg, lexicon = "bing", negation_words))
(str.NegSentiment(myPositiveText, replace_reg, lexicon = "bing", negation_words))  
(str.NegSentiment(myNegPositiveText, replace_reg, lexicon = "bing", negation_words))  
(str.NegSentiment(myPosNegativeText, replace_reg, lexicon = "bing", negation_words))  

(str.NetSentiment(myNegativeText, replace_reg, lexicon = "bing", negation_words))
(str.NetSentiment(myPositiveText, replace_reg, lexicon = "bing", negation_words))  
(str.NetSentiment(myNegPositiveText, replace_reg, lexicon = "bing", negation_words))  
(str.NetSentiment(myPosNegativeText, replace_reg, lexicon = "bing", negation_words))  


complaints$narrative[4567]

```

\ 


### 6. Summary 

The shiny app demo will be used to demonstrate how sentiment differs between products and whether the user was compensated.

The app makes use of pre-processed data from this markdown document to make the app more responsive and to avoid re-computation as far as possible.

The app consists of 3 panels each aimed at a seperate functionality of the project brief.

1. Tab 1: *Sentiment analysis* 
2. Tab 2: *Topic modeling* 
3. Tab 3: *New complaint* 
\ 

#### Sentiment analysis tab 

The user can select any combination of products and compensation variables, and the histograms will update accordingly.


#### Topic modeling tab

Topic moddeling by default will be based on selection of all products and compensation variable setting. Due to the runtime required to perform LDA, a seperate tick-box was provided to enable / disable the LDA modelling computaions to take place. This was implemented specifically with the demo in mind.


#### New complaint tab 

Not fully functional.
At this stage a user is allowed to enter a new complaint and a sentiment score for this text will be calculated.

Relative ranking and topic probability of the new complaint is not implmented yet.



### 7. References  

[1] DATA.GOV - Bureau of Consumer Financial Protection 
 - Consumer Complaint Database. [CFPB](https://catalog.data.gov/dataset/consumer-complaint-database)
[2] TMR [TMR](http://tidytextmining.com/) 
[3] R4DS [R4DS](http://r4ds.had.co.nz/) 


